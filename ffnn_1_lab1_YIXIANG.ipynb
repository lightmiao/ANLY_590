{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ffnn_1_lab.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"jUI9Jbekte4t","colab":{},"executionInfo":{"status":"ok","timestamp":1599535608216,"user_tz":240,"elapsed":859,"user":{"displayName":"Yi Xiang","photoUrl":"","userId":"15699701909452907654"}}},"source":["import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eSL4ugW8uEUt"},"source":["**intro + NumPy**\n","\n","In this lab we will implement the forward pass of a neural network. We will rely heavily on NumPy -- a Python package for efficienct processing of array data.\n","\n","To get started with this, here is an implementation of a function that computes the sigmoid activation function using NumPy to help us compute the exponential. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_2HxaPkguD5h","colab":{},"executionInfo":{"status":"ok","timestamp":1599535610455,"user_tz":240,"elapsed":523,"user":{"displayName":"Yi Xiang","photoUrl":"","userId":"15699701909452907654"}}},"source":["def sigmoid(z):\n","  '''\n","  Sigmoid activation function\n","\n","  parameters:\n","  - z (array): input to the activation function\n","  '''\n","  return 1 / (1 + np.exp(-np.array(z)))"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j8EkDVrFuwtf"},"source":["One great thing about using NumPy is that often times we can write code that looks like it operates on a single number, but NumPy will also work on its own `ndarray` array objects (nd = 'n-dimensional') as well as lists (which it will convert to arrays for you.\n","\n","Try it out before. Use this function to compute the output of the sigmoid activation function for the input value 0.0 by itself, and then also for the values (-2.0, -1.0, 0.0, 1.0, 2.0) at the same time with only one call to the function."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rUKdaVZ5ve9L","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599535614807,"user_tz":240,"elapsed":583,"user":{"displayName":"Yi Xiang","photoUrl":"","userId":"15699701909452907654"}},"outputId":"fcf0b03a-c49e-46f0-e96a-37e2633b3ac9"},"source":["# your code here\n","\n","print(sigmoid([-2.0, -1.0, 0.0, 1.0, 2.0]))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6WUgTplRvr56"},"source":["**single neuron**\n","\n","As a warm-up for implementing a full neural network layer, let's implement the computation performed by a single neuron.\n","\n","If our neuron has $m$ inputs, then its output will be defined by:\n","\n","- the inputs, which we can represent as an $m$-dimensional array: $x=[x_1, x_2, ..., x_m]$\n","- the weights for each input, which we can represent as another $m$-dimensional vector: $w = [w_1, w_2, ..., w_n]$\n","- the bias, which is a scalar: $b$\n","\n","Use the sigmoid activation function -- $\\sigma(z)$ -- for this neuron.\n","\n","With these definitions, the output of our neuron is:\n","$$a = \\sigma(w_1 x_1 + w_2 x_2 + ... + w_m x_m + b)$$\n","$$=\\sigma(w \\cdot x + b)$$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MXN-vL7owPRS","colab":{},"executionInfo":{"status":"ok","timestamp":1599535617070,"user_tz":240,"elapsed":583,"user":{"displayName":"Yi Xiang","photoUrl":"","userId":"15699701909452907654"}}},"source":["def neuron(x, w, b):\n","    z=0\n","    for i in range(len(x)):\n","        z+=x[i]*w[i]\n","        \n","    z+=b\n","    return sigmoid(z)\n","\n","def neuron(x, w, b):\n","  return sigmoid(np.sum([x_i*w_i for x_i,w_i in zip(x,w)])+b)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rfrP9HZGy2cS"},"source":["Here are some tests that should pass if your implementation is correct."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YVNiKA47y1bC","colab":{},"executionInfo":{"status":"ok","timestamp":1599535619265,"user_tz":240,"elapsed":420,"user":{"displayName":"Yi Xiang","photoUrl":"","userId":"15699701909452907654"}}},"source":["np.testing.assert_equal(\n","    neuron([0, 0, 0], [0, 0, 0], 0),\n","    0.5\n",")\n","np.testing.assert_equal(\n","    neuron([0, 0, 0], [0, 0, 0], 1),\n","    1 / (1 + np.exp(-1))\n",")\n","np.testing.assert_equal(\n","    neuron([1, 0, 1], [0, 1, 0], 0),\n","    0.5\n",")\n","np.testing.assert_equal(\n","    neuron([1, -1, 0], [2, 1, 0], -1),\n","    0.5\n",")"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Qgo1Amx_wbhU"},"source":["**vectorizing**\n","\n","If your code for a single neuron used a loop, you had the right idea! However, in languages with Python, which are not built for speed, writing your own loops can be slow. Right now, your code probably doesn't feel very slow, but once we start to fit your neural net to data, you will need to call this function many times (easily millions of times, if not more!).\n","\n","For critical portions of your code like this, it can be worth the extra work to *vectorize* your code. This means rewriting your code to use a package like NumPy that can compute the result you want directly from vectors/arrays, allowing NumPy to do the looping under the hood for you. The benefit here is that NumPy's has critical loops written in lower level languages (like C, C++, or FORTRAN) where loops are faster (though the code is harder to write and make sure it is correct).\n","\n","Go back and try to write a vectorized version of the `neuron` function using helper functions from NumPy that let you avoid writing a loop. Hint: check out the NumPy's `np.dot` function."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NEorVEyNy-23","colab":{},"executionInfo":{"status":"ok","timestamp":1599535622933,"user_tz":240,"elapsed":766,"user":{"displayName":"Yi Xiang","photoUrl":"","userId":"15699701909452907654"}}},"source":["# vectorized version\n","def neuron(x, w, b):\n","  return sigmoid(np.dot(x,w)+b)\n","  '''\n","  tranformation for a single layer of a neural network\n","\n","  parameters\n","  ----------\n","  x (1d array): input vector\n","  w (1d array): neuron weights\n","  b (float):    bias\n","  '''\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"elFSDgDI3Nwm"},"source":["Try the tests again to make sure it still works."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6N06NPgL3R_O","colab":{},"executionInfo":{"status":"ok","timestamp":1599535624667,"user_tz":240,"elapsed":447,"user":{"displayName":"Yi Xiang","photoUrl":"","userId":"15699701909452907654"}}},"source":["np.testing.assert_equal(\n","    neuron([0, 0, 0], [0, 0, 0], 0),\n","    0.5\n",")\n","np.testing.assert_equal(\n","    neuron([0, 0, 0], [0, 0, 0], 1),\n","    1 / (1 + np.exp(-1))\n",")\n","np.testing.assert_equal(\n","    neuron([1, 0, 1], [0, 1, 0], 0),\n","    0.5\n",")\n","np.testing.assert_equal(\n","    neuron([1, -1, 0], [2, 1, 0], -1),\n","    0.5\n",")"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7ji8uJNCzFUQ"},"source":["**neural net layer with batch inputs**\n","\n","Now for the real deal: extending our function for a single neuron in two ways:\n","- first, so that it can process a \"batch\" of multiple inputs rather than just a single input\n","- second, so that it can do the computation for an entire hidden layer of neurons instead of just one\n","\n","Let's look at the math for each of these extensions in turn.\n","\n","*batch inputs*:\n","\n","We want to process a set (or \"batch\") of inputs at the same time, both for the sake of convenience and so we can hopefully vectorize the code to make processing a batch faster than simply looping through all of the inputs in the batch.\n","\n","If we have $k$ inputs in a batch, each of which are $m$-dimensional, then we can organize these inputs into a $k$-by-$m$ matrix simply by stacking the individual input vectors:\n","\n","\\\\\n","$$\n","\\begin{bmatrix}\n","  -\\, x^1 \\, -\\\\\n","  -\\, x^2 \\, -\\\\\n","  \\vdots \\\\\n","  -\\, x^k \\, -\\\\\n","\\end{bmatrix}\n","=\n","\\begin{bmatrix}\n","x_{11} & x_{12} & \\cdots & x_{1m} \\\\\n","x_{21} & x_{22} & \\cdots & x_{2m} \\\\\n","\\vdots & \\vdots  &  \\ddots & \\vdots \\\\\n","x_{k1} & x_{k2} & \\cdots & x_{km}\n","\\end{bmatrix}\n","$$\n","\n","\\\\\n","where each row contains the components for one input\n","\n","*multiple neurons*\n","\n","Now let's considering an entire layer of neurons that recieve this same input and, again, we want to compute the outputs for the entire layer together for convenience and efficiency.\n","\n","If we have $n$ neurons, we will now have $m$ of weights (for a  $m$-dimensional input) for each neuron. Similar to what we just saw for extending to multiple inputs, we can collect these weights in an $m$-by-$n$ matrix, but this times lets put each weight vector in its own column (we'll see why shortly):\n","\n","\\\\\n","$$W =\n","\\begin{bmatrix}\n","| & | & \\quad & | \\\\\n","w^1 & w^2 & \\cdots & w^n \\\\\n","| & | & \\quad & |\n","\\end{bmatrix}\n","=\n","\\begin{bmatrix}\n","w_{11} & w_{12} & \\cdots & w_{1n} \\\\\n","w_{21} & w_{22} & \\cdots & w_{2n} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","w_{m1} & w_{m2} & \\cdots & w_{mn}\n","\\end{bmatrix}\n","$$\n","\n","\\\\\n","where each column contains the weights for one of the neurons in the layer. We also have one bias for each neuron, which we can organize in a vector:\n","\n","\\\\\n","$$\n","b =\n","\\begin{bmatrix}\n","b_1 & b_2 & \\cdots & b_n\n","\\end{bmatrix}\n","$$.\n","\n","\\\\\n","*putting it all together*\n","\n","Next, we want to compute the total weighted input to each neuron in the layer for each input. We can organize these values into a matrix, with one row for each sample and one column for each neuron in our layer. \n","\n","\\\\\n","$$\n","\\begin{bmatrix}\n","x^1 \\cdot w^1 & x^1 \\cdot w^2 & \\cdots & x^1 \\cdot w^n \\\\\n","x^2 \\cdot w^1 & x^2 \\cdot w^2 & \\cdots & x^2 \\cdot x^n \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","x^m \\cdot w^1 & x^m \\cdot w^2 & \\cdots & x^m \\cdot w^n\n","\\end{bmatrix}\n","$$\n","\n","\\\\\n","But taking the dot product between all rows of one matrix with all columns of another matrix is exactly the definition of matrix multiplication, so we have:\n","\n","\\\\\n","$$\n","\\begin{bmatrix}\n","x^1 \\cdot w^1 & x^1 \\cdot w^2 & \\cdots & x^1 \\cdot w^n \\\\\n","x^2 \\cdot w^1 & x^2 \\cdot w^2 & \\cdots & x^2 \\cdot x^n \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","x^m \\cdot w^1 & x^m \\cdot w^2 & \\cdots & x^m \\cdot w^n\n","\\end{bmatrix}\n","=\n","\\begin{bmatrix}\n","  -\\, x^1 \\, -\\\\\n","  -\\, x^2 \\, -\\\\\n","  \\vdots \\\\\n","  -\\, x^k \\, -\\\\\n","\\end{bmatrix} \n","\\begin{bmatrix}\n","| & | & \\quad & | \\\\\n","w^1 & w^2 & \\cdots & w^n \\\\\n","| & | & \\quad & |\n","\\end{bmatrix}\n","=XW\n","$$\n","\n","\\\\\n","We also need to add the bias terms, which is the same for each row/neuron. We can write this as:\n","\n","\\\\\n","$$\n","\\begin{bmatrix}\n","x^1 \\cdot w^1 + b_1 & x^1 \\cdot w^2 + b_2 & \\cdots & x^1 \\cdot w^n + b_n \\\\\n","x^2 \\cdot w^1 + b_1 & x^2 \\cdot w^2 + b_2 & \\cdots & x^2 \\cdot x^n + b_n \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","x^m \\cdot w^1 + b_1 & x^2 \\cdot w^2 + b_2 & \\cdots & x^m \\cdot w^n + b_n\n","\\end{bmatrix} \\\\\n","=\n","\\begin{bmatrix}\n","x^1 \\cdot w^1 & x^1 \\cdot w^2 & \\cdots & x^1 \\cdot w^n \\\\\n","x^2 \\cdot w^1 & x^2 \\cdot w^2 & \\cdots & x^2 \\cdot x^n \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","x^m \\cdot w^1 & x^m \\cdot w^2 & \\cdots & x^m \\cdot w^n\n","\\end{bmatrix}\n","+\n","\\begin{bmatrix}\n","b_1 & b_2 & \\cdots & b_m\n","\\end{bmatrix}\n","= XW + b\n","$$\n","\n","\\\\\n","where when have have addition of a matrix to a row/column vector, we interpret this via *broadcasting*: we assume that the row/column is repeated across all rows/columns to match the shape of the matrix. This is exactly how many numeric programming langauges/packages (including NumPy) handle this as well.\n","\n","Finally, the outputs for all neurons across all batches can be computed by applying the activation function elementwise for a final result of:\n","\n","\\\\\n","$$\n","A = \\sigma(XW + b)\n","$$\n","\n","\\\\\n","And here is the best part: the same function in NumPy that computes the dot-product between two 1-dimensional arrays also computes matrix multiplication when given 2-dimensional arrays instead. This means that the function you wrote for a single neuron will already work for multiple neurons across a batch of inputs!\n","\n","Go ahead and try it -- simply change the weights and inputs to 2d-arrays (and you can change the biases to a 1d-array if you want a different bias for each neuron."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D_sH6F9WzMvD","colab":{},"executionInfo":{"status":"ok","timestamp":1599535629073,"user_tz":240,"elapsed":458,"user":{"displayName":"Yi Xiang","photoUrl":"","userId":"15699701909452907654"}}},"source":["def nn_layer(X, W, b):\n","  return sigmoid(np.dot(X,W)+b)\n","\n","  '''\n","  tranformation for a single layer of a neural network\n","\n","  parameters\n","  ----------\n","  X (2d array): input vectors\n","  W (2d array): neuron weights\n","  b (1d array): biases\n","  '''\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"t3UTPy9QzRGB"},"source":["Again, here are some tests to see if your network is probably handling multiple inputs and multiple outputs."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pJjbo6QOzW9s","colab":{},"executionInfo":{"status":"ok","timestamp":1599535633257,"user_tz":240,"elapsed":396,"user":{"displayName":"Yi Xiang","photoUrl":"","userId":"15699701909452907654"}}},"source":["X = [\n","     [0, 0, 0],\n","     [1, 1, 1],\n","     [-1, 0, 1]\n","]\n","W = [\n","     [0, 1, 0],\n","     [0, 1, 1],\n","     [0, 1, 2]\n","]\n","b = [0, 0, 0]\n","A = nn_layer(X, W, b)\n","np.testing.assert_array_equal(A, [[0.5, 0.5, 0.5], [0.5, sigmoid(3), sigmoid(3)], [0.5, 0.5, sigmoid(2)]])\n","\n","b = [[1, 2, 3]]\n","A = nn_layer(X, W, b)\n","np.testing.assert_array_equal(A, [[sigmoid(1), sigmoid(2), sigmoid(3)], [sigmoid(1), sigmoid(5), sigmoid(6)], [sigmoid(1), sigmoid(2), sigmoid(5)]])"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mEXH8yhKzc4z"},"source":["**neural net with one hidden layer**\n","\n","Next, let's make a function to implement a two-layer neural network - one hidden layer and the output layer. It should takes as inputs:\n","- An input matrix, $X$\n","- Two weight matrices, $W_1$ and $W_2$ (one for each layer)\n","- Two bias vectors, $b_1$ and $b_2$ (one for each layer)\n","\n","This function should call your `nn_layer` function twice with the appropriate arguments, chaining the output of the first layer into the input to the secocond layer, and then returning the final output"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rD96HaR4zkW3","colab":{},"executionInfo":{"status":"ok","timestamp":1599535635100,"user_tz":240,"elapsed":425,"user":{"displayName":"Yi Xiang","photoUrl":"","userId":"15699701909452907654"}}},"source":["def nn_two_layers(X, W_1, b_1, W_2, b_2):\n","  '''\n","  forward pass of a two-layer neural network\n","\n","  parameters\n","  ----------\n","  X (2d array):   input matrix\n","  W_1 (2d array): weight matrix for first layer\n","  b_1 (1d array): biases for first layer\n","  W_2 (2d array): weight matrix for second layer\n","  b_2 (1darray):  biases for second layer\n","  '''\n","  H=nn_layer(X,W_1,b_1)\n","  Y=nn_layer(H,W_2,b_2)\n","  return Y"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"J9xr7V-hzqGi"},"source":["To test out your brand new neral net function, let's test it out on a 1-dimensional input and a 1-dimensional output so that we can plot the results with a line graph. We will use random values for the weight and biases.\n","\n","Follow along below as we walk through how to do this with some NumPy functions and Python's popular plotting package, Matplotlib."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"euPE7moLzuSI","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1599535638768,"user_tz":240,"elapsed":622,"user":{"displayName":"Yi Xiang","photoUrl":"","userId":"15699701909452907654"}},"outputId":"feadd41b-a6d8-4556-cf22-587339d05094"},"source":["# Set the input dimension, the number of hidden units, and the number of ouptput units\n","n_input, n_hidden, n_output = 1, 20, 1\n","\n","# We want get the NN's output for a range of input values, so that we cant plot\n","# input vs output. We can get evenly space values using `np.linspace`. We also\n","# want to process these inputs as a \"batch\", so we use `np.newaxis` to turn this\n","# 1-d array into a 2-d array with a single column.\n","n_grid = 1000\n","x_left, x_right = -100, 100\n","X = np.linspace(x_left, x_right, n_grid)[:, np.newaxis]\n","\n","# We can generate random values (drawn from a standard gaussian distribution --\n","# mean = 0, standard deviation = 1), with `np.random.randn(shape)`\n","W_1 = np.random.randn(n_input, n_hidden)\n","b_1 = np.random.randn(n_hidden)\n","W_2 = np.random.randn(n_hidden, n_output)\n","b_2 = np.random.randn(n_output)\n","\n","# Use our NN to compute the outputs for these inputs\n","Y_hat = nn_two_layers(X, W_1, b_1, W_2, b_2)\n","\n","# Plot the input values (x-axis) against the output values (y-axis)\n","plt.plot(X, Y_hat)\n","plt.show()"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXRb533m8e8PAAFwJ0VSJCXS2ixr8RpZVp3abpw4ju0stpM0qTNdkiZt2k6c5aSdqVvP8fS450zHTZNpe8bTxtMmTZo6dtomjZw6dp1M7CROHFuyZdnal0giRUoixZ0gCAJ45w+AEiRT4gbgYnk+5+Dg3hcXwO9cQI9evve9F+acQ0REip/P6wJERCQ7FOgiIiVCgS4iUiIU6CIiJUKBLiJSIgJevXFzc7NbuXKlV28vIlKUtm/f3u+ca5npMc8CfeXKlWzbts2rtxcRKUpmdvRCj2nIRUSkRCjQRURKhAJdRKREKNBFREqEAl1EpEQo0EVESoQCXUSkRHg2D11ELmw4MsWu3mGOnY4wNDFFLJ7EgJpwgJpQgMaqIG31YZY1VNJYVYGZeV2yFAAFukiBiE4l+PaO43z9xS5e7R5irj9VEK7w0V5fyfKGSjoap29VLE8vL60N4/cp8MuBAl2kADy3v4/7v/Ua3YMTrG+r5dO3rGXziiWsaKqiqSZIKOAnkXSMT8YZm4xzejxG79AEPcPR9P0Ex4eifG/PSfrHYue8doXfaK8/L+ynw39JFa21IQJ+jb6WAgW6iIecc3zxh4d56Km9rGmp4Wsf+wVuuLRpxiEUv88IBoI0VgfpXFLFNZ0NM77mRCzB8aEJugcjdA9OpJdT68/u6+PU6OQbXre9PszyhkpaakM014TS90GaqkM0p5eba0KEK/w52Q+SHQp0EQ898sPD/M/v7uU9Vy/jc798VVYCszLo59KlNVy6tGbGx6NTCXqGzg364+ng39UzQv/oJKOT8RmfWxMKUF9ZQV1lBfWVAerCFdRXnr3Vpe9rwwGqggGqgn6qQ/4zy1XBAMGA/hrIFQW6iEee2X2SP/vuXt59VTt/9SvX4MvTOHe4ws/qlhpWt8wc+JAK/f6xSfrHYvSPTqaXJzk9HmN4YoqRiSlGJuIcPR1heGKK4YkpJqYSc3r/Cr9RWeGnOnQ25EMBH6EKH0G/j2DARyjgJxiYXj7bFgqktpneNuD3EfAZfp+dua/w+85ZD/gNvy+1XcA/3f7G5/nMMAMzw2fgs8y2s+u+9DaFSIEu4oH+sUnu+9edXL6sjs9/8Oq8hflchSv8dDRW0dFYNefnxOJJRqKpcB+NxonE4kzEEozHEkQm40RiCSKx6fvU8ngswUQswWQ8weRUktFonMmpJLFEklg8mWqPJ5mMp9YLydmQP/ufgHH2PwMz8PkMY3p9elv4w9vX875NHVmvSYEu4oE/e3Ivo9E4j/72NYQCpTEuHQz4aK5JjcHngnPuTNDH0iGfSDriSUcimWQq4c5Zj2esx89bn+l5zjmSDpLp6UXJjHXnOOfxZHrdnbeePNM2/bz0Y2Rsk4RlDZU52UcKdJE823tihG++0s1v37SadW21XpdTNMwsPexSGv8B5oKOTojk2Rf+Yz81oQD/+eY1XpciJUaBLpJHXQMRntlzkg+/eSUNVUGvy5ESo0AXyaOvvXAUnxm/ev0lXpciJUiBLpInsXiSx7d18Y6NrbTX5+agmJQ3BbpInjx/sJ+hyBQf2Jz96WoiMMdAN7PbzWyfmR00s/tmePwjZtZnZjvSt9/Kfqkixe2JnT3UhQPceGmL16VIiZp12qKZ+YGHgVuBbuAlM9vqnNt93qaPO+fuzUGNIkVvMp7gmV0nuf2KNp36Ljkzl2/WFuCgc+6wcy4GPAbclduyRErLC4cHGJ2M884r270uRUrYXAJ9OdCVsd6dbjvf+81sp5n9i5l1zvRCZvZxM9tmZtv6+voWUK5IcfrR/j6CAR/Xr27yuhQpYdn62+8JYKVz7irgGeArM23knHvEObfZObe5pUXjiFI+fnSgny0rl1AZ1FmOkjtzCfTjQGaPuyPddoZz7rRzbvoiy38HXJud8kSK38mRKPtOjnLT2mavS5ESN5dAfwlYa2arzCwI3ANszdzAzDIHBu8E9mSvRJHi9uMD/QDcqECXHJt1lotzLm5m9wJPA37gS865XWb2ILDNObcV+JSZ3QnEgQHgIzmsWaSobDs6QF04wIa2Oq9LkRI3p6stOueeBJ48r+2BjOU/Av4ou6WJlIbtRwfZtKKx4K55LqVHE2JFcmh4YooDp8bYdEmj16VIGVCgi+TQjq4hnINrVyjQJfcU6CI5tP3oID6DqzsbvC5FyoACXSSHdnYPcVlrLTUh/TiY5J4CXSSHdveMsHGZZrdIfijQRXKkb3SSU6OTXL6s3utSpEwo0EVyZHfvCAAb29VDl/xQoIvkyK6eYQANuUjeKNBFcmR3zwgdjZXUV1Z4XYqUCQW6SI7s6R1hg4ZbJI8U6CI5EIsnOXI6wrrWWq9LkTKiQBfJgWMD4ySSjjVLq70uRcqIAl0kBw6eGgPg0hb10CV/FOgiOTAd6Ktb1EOX/FGgi+TAwVNjLKsPU61T/iWPFOgiOXCob5w1S2u8LkPKjAJdJMuSScehvjHWtCjQJb8U6CJZ1jsSJRJLcKl66JJnCnSRLDs0PcNFgS55pkAXybKf948DsLpZM1wkvxToIll2bCBCZYWfltqQ16VImVGgi2TZsYEIlyypwsy8LkXKjAJdJMuOnY7QuaTK6zKkDCnQRbLIOXemhy6Sbwp0kSzqH4sxMZXgkiWVXpciZUiBLpJFxwYiAKxo0gwXyT8FukgWdaUDXWPo4gUFukgWHT2dCvSORg25SP4p0EWy6NhAhLa6MOEKv9elSBlSoItkUddAhEuaNNwi3lCgi2SRpiyKl+YU6GZ2u5ntM7ODZnbfRbZ7v5k5M9ucvRJFikN0KsGJkagCXTwza6CbmR94GLgD2Ah8yMw2zrBdLfBp4GfZLlKkGHQPpg6IKtDFK3PpoW8BDjrnDjvnYsBjwF0zbPenwENANIv1iRSN7sEJAJZrhot4ZC6BvhzoyljvTredYWabgE7n3L9f7IXM7ONmts3MtvX19c27WJFC1juc6sssa1CgizcWfVDUzHzAF4Dfn21b59wjzrnNzrnNLS0ti31rkYLSOzSBz6BVl80Vj8wl0I8DnRnrHem2abXAFcCzZnYEuB7YqgOjUm56hqMsrQ0T8GvymHhjLt+8l4C1ZrbKzILAPcDW6Qedc8POuWbn3Ern3ErgBeBO59y2nFQsUqB6hiZY1hD2ugwpY7MGunMuDtwLPA3sAb7hnNtlZg+a2Z25LlCkWPQOR2nX+Ll4KDCXjZxzTwJPntf2wAW2vXnxZYkUF+ccPUMTvH3DUq9LkTKmwT6RLBiMTDEZT9Jerx66eEeBLpIFPUOpOeiasiheUqCLZMHZQNdBUfGOAl0kC6ZPKtKQi3hJgS6SBT1DEwQDPpqqg16XImVMgS6SBT3DUdrrw/h85nUpUsYU6CJZ0Ds0QXu9xs/FWwp0kSzoHY6yTOPn4jEFusgiJZKOEyNRTVkUzynQRRbp1GiURNLRrimL4jEFusgi9Qylr4OuIRfxmAJdZJF6h3WWqBQGBbrIIk2fJaohF/GaAl1kkXqGotSEAtSFK7wuRcqcAl1kkXqHNQddCoMCXWSReoY0ZVEKgwJdZJF6h/XTc1IYFOgiizAZT9A/FtNVFqUgKNBFFuFE+rK5GnKRQqBAF1mE49M/bKGDolIAFOgii9CbPku0XT10KQAKdJFFmD5LVNMWpRAo0EUWoWc4SlN1kHCF3+tSRBToIovRMzShU/6lYCjQRRahdyiqKYtSMBToIovQMzzBch0QlQKhQBdZoNHoFKPRuA6ISsFQoIssUO+wpixKYVGgiyzQ9HXQl+ugqBQIBbrIAk3/9JwOikqhUKCLLFDv8AR+n7G0NuR1KSLAHAPdzG43s31mdtDM7pvh8d81s9fMbIeZ/djMNma/VJHCcnxogtbaEAG/+kVSGGb9JpqZH3gYuAPYCHxohsB+1Dl3pXPuGuDPgS9kvVKRAtOrH7aQAjOXrsUW4KBz7rBzLgY8BtyVuYFzbiRjtRpw2StRpDD1DE9ohosUlLkE+nKgK2O9O912DjP7hJkdItVD/9RML2RmHzezbWa2ra+vbyH1ihSEZNLROxzVLxVJQcna4J9z7mHn3BrgD4H/doFtHnHObXbObW5pacnWW4vk3enxGLF4kmWa4SIFZC6BfhzozFjvSLddyGPA3YspSqTQTV82V2PoUkjmEugvAWvNbJWZBYF7gK2ZG5jZ2ozVdwEHsleiSOGZPqlIp/1LIQnMtoFzLm5m9wJPA37gS865XWb2ILDNObcVuNfM3g5MAYPAh3NZtIjXpk8q0oW5pJDMGugAzrkngSfPa3sgY/nTWa7rgl4/PszLxwYJ+HxU+I0Kv48Kv4+A3whmLKfazz4+vZy5XTCQuheZr56hCcIVPhqqKrwuReSMOQV6IfnRgX4eempv1l4v4DMqK/yEKvxUBn1UVvgJp2+V6Vu4wkdl8GxbTThAbbiCunCA2vRybThATSi9HArg81nWapTC0zM8wbKGSsz0OUvhKLpA/80bVvLBzR1MJRxTiWT6dnY5nnRMxZNMTd8nzi7Hk0liibPtsXiSiakE0anp+wQTsQTReOp+KBLjRPqxiakE0ViCyFSCRHL2afY1oQB14QCN1UGWVAdpqg6ypDpEU01qfbqtqSZEa12IqmDRfRRlrWcoqhkuUnCKLkWme89ecc4RnUoyGp1iJBpnNDrF2GSc0fRy6j51G56YYjAS4/R4jCOnxxkYizEeS8z4uvWVFbTVhWmrD9Nen7qfXu9cUkVnYxXBgIaHCkXP0AQ3r9PUWyksRRfoXjMzKoN+KoN+ltbN//nRqQQD4zEGxlNBf3pskhMjUU4MR+kdjnJyJMru3hH6xyZxGX8I+Cx1Vb+VzVVcsqSaFU1VrGyqYnVLDauaq3UsII9i8SR9Y5O6yqIUHAV6noUr/CxrqJx1/nIsnuTUaCrojw1EOHI6wrHT4xw5HeHpXScYGI+d2bbCb6xqruay1lrWtdZyWVvq/pIlVRrLz4GTI1Gc0wwXKTwK9AIVDPjoaKyio7GKzSuXvOHxkegUR/sjHOobY9/JUQ6cHOXV7iG+s7P3zDa1oQBXLK/nqo56rupo4KqOejoadSBvsc7MQddp/1JgFOhFqi5cwZUd9VzZUX9O+/hknAOnxth3YoTXjg/zWvcwX37+CLFEEoAl1UGu7qhny6omtqxawpXL6zU2P0/dg9O/VKQeuhQWBXqJqQ4FuKazgWs6G/iV61Jtk/EE+06MsrN7mJ3dQ2w/OsgP9qUujlZZ4WfTiga2rGziF1YvYdMljQr4WXQNRjCD5Y0KdCksCvQyEAr400MuDcAKAPrHJnnp5wO8eGSAF38+wF9+fz/ue6nplm9e08RbLmvhLZe10LmkytviC1DXwASttWFCAe9mW4nMRIFeppprQtxxZTt3XNkOwPDEFC8cPs0P9/fx7L4+ntl9EoDVzdW8ZV0Lt13exnUrl+DXQVa6BiN0LlHvXAqPAl2A1Dz42y5v47bL23DOcbh/nOf29fHc/j7+6WfH+PLzR2iqDnLrxlZuu6KNX1zTVLY91O6BCNevbvK6DJE3UKDLG5gZa1pqWNNSw0dvXMX4ZJxn9/Xx9K4TfGdnL4+91EVNKMAtG5Zy1zXLuGltS9nMg4/Fk/SOROnQUJQUIAW6zKo6FOBdV7XzrqvamYwn+MnB0zz1+gme3n2Cb+/ooak6yHuuXsbdb1rO1R31JT0tsmdoAuegUwdEpQAp0GVeQgE/b12/lLeuX8qfxq/guf19/Nsrx3n0xWP8w0+OsKq5mruvWc4vb+4oyWl9XYMRAB0sloKkQJcFCwZ83LqxlVs3tjISneKp107wrVeO85ff389ffX8/N69byn/acgk3r2shUCJDMl0DqTnoCnQpRAp0yYq6cAUfvK6TD17XSddAhMdf6uLxbV381le30V4f5oObO7lnS2fRX/+kazBChd9oq9NZolJ4SqPbJAWlc0kVf3DbOn5y39v421+7lrWttfz1/zvAjQ/9gE9+/RV2dA15XeKCdQ1EWNZQqembUpDUQ5ecqfD7uP2KNm6/oo2ugQj/+MJRvv6zYzzxag+bVzTyWzet4taNbUUVjl2DE3Q2arhFCpN66JIXnUuq+ON3buCnf3wLD7x7IydGovzu117mrX/xLI+9eIyp9LVmCplzjp/3jbGiSYEuhUmBLnlVEwrw0RtX8dx/eSt/86ubaKiq4L5vvlYUwX56PMZINM7qlhqvSxGZkQJdPOH3GXdc2c63P3EDX/7IdTRVB88E+zdf7iY5h5/5y7fDfeMArG6p9rgSkZkp0MVTZsZb1y/l3z5xA1/+zetorAry2W+8ynv/z/NsPzrgdXnnONw3BsCaZvXQpTAp0KUgmBlvXbeUb3/iBj7/gas5MRLl/X/zUz7z2Cvn/DqTlw73jxMM+HTZXClYCnQpKD6f8f5rO/jBH9zMJ992Kf/+Wi+3fuE5vrOzB+e8HYY53DfGyqaqopqVI+VFgS4FqSoY4PffsY4nPnkjyxsruffRV/jM4zuIxOKe1XS4b5zVGm6RAqZAl4K2vq2Ob/7eL/LZWy/jiVd7uPvh5zmUHsvOp6lEkmMDER0QlYKmQJeCF/D7+NQta/nqR3+B/rEYd/3v5/npodN5reHAyTHiSce6ttq8vq/IfCjQpWjcuLaZJz55I231YT785Rd56vUTeXvvXT3DAFy+rH6WLUW8o0CXorK8oZJ//p03s7G9jnsffZln953Ky/vu6hmhKuhnVbOGXKRwKdCl6DRWB/nqx7ZwWWstv/e1l9l+dDDn77mrZ5gN7XWa4SIFTYEuRakuXMFXPrqF1roQv/OP2zk1Es3Ze8UTSXb3jHDFsrqcvYdINswp0M3sdjPbZ2YHzey+GR7/rJntNrOdZvZ9M1uR/VJFztVSG+KR39jM+GScex99hXiOrgPzavcw47EEW1bph6GlsM0a6GbmBx4G7gA2Ah8ys43nbfYKsNk5dxXwL8CfZ7tQkZlc1lrL/3jfFbx4ZIC///HPc/IePz7QD8Cb1yjQpbDNpYe+BTjonDvsnIsBjwF3ZW7gnPuBcy6SXn0B6MhumSIXdvc1y3nHxla+8Mx+jvSPZ/W1nXN8e8dxNq9oZEl1MKuvLZJtcwn05UBXxnp3uu1CPgZ8d6YHzOzjZrbNzLb19fXNvUqRizAzHrzrCoJ+Hw9s3ZXV1/7u6yc43D/Or1zXmdXXFcmFrB4UNbNfAzYDn5vpcefcI865zc65zS0tLdl8aylzbfVhPnXLWn64v4+fHOzPymtGYnEefGI3ly+r471vulgfRqQwzCXQjwOZ3ZOOdNs5zOztwP3Anc65yeyUJzJ3v/7mFSyrD/PQU3uzciGvf3ulhxMjUf77ey4n4NeEMCl8c/mWvgSsNbNVZhYE7gG2Zm5gZm8CvkgqzPNzpofIecIVfu5921pe7R7mp4cXf2mAnx4+TXt9mOtWNmahOpHcmzXQnXNx4F7gaWAP8A3n3C4ze9DM7kxv9jmgBvhnM9thZlsv8HIiOfW+Tctprgnydz9a/IyXg6fG2NBeh5lOJpLiEJjLRs65J4Enz2t7IGP57VmuS2RBwhV+fv36lfyv7+3ncN/Ygn//0znH0dPjXL96SZYrFMkdDQxKyfnQlk78PuNftncv+DUisQSRWILWunAWKxPJLQW6lJyldWHeclkL33z5OIkF/tj0YCT1s3eNVRXZLE0kpxToUpI+cG0HJ0ai/HiBUxiHIlMANFTpZCIpHgp0KUlv27CUmlCAp17vXdDzzwR6pXroUjwU6FKSQgE/N69r4ZndJxc07HJmyEWn+0sRUaBLybrt8jb6x2K8fGz+10sfSgd6g8bQpYgo0KVk3byuhaDfxzO7T877uYNnhlzUQ5fioUCXklUbrmDzykZ+dGD+B0aHIlPUhAIEA/onIsVD31YpaTdc2sye3hH6x+Z3eaGR6BR14TmddydSMBToUtJuuLQZgJ8emt+1XSKxONUhBboUFwW6lLQrl9dTGw7w/Dzno49PJqhSoEuRUaBLSfP7jOtXN/HCPK++GInFqQ76c1SVSG4o0KXkXbuikSOnI5yexzj6+GSCqqB66FJcFOhS8jZdkrqe+SvHhub8nNQYunroUlwU6FLyruqoJ+CzeZ1gNB5TD12KjwJdSl64ws/GZXXzCvTIpMbQpfgo0KUsbLqkkVe7hoknkrNum0w6IlOa5SLFR4EuZeGqjnomphIc7h+fddtoPIFzqIcuRUeBLmXh8mX1AOzqGZ512/HJBIB66FJ0FOhSFta0VBMK+Nh1fGTWbSOxOKAeuhQfBbqUhYDfx/q2Wnb1zB7oZ3romuUiRUaBLmVj47J6dvUM49zFf/BiuodepR66FBkFupSNy5fVMRKN0z04cdHtxmOpHrouziXFRoEuZePyZXUAsw67RCbTY+g6U1SKjAJdysb6tjp8BrtnmelypoeuMXQpMgp0KRuVQT+rmqvZc2L0ottpDF2KlQJdysqG9jr29F58yGV6lovG0KXYKNClrGxor6N7cIKR6NQFt4nE4vgMQvo9USky+sZKWVnfVgvA/osMu4xPJqgOBjCzfJUlkhUKdCkrG9pTM10uNuwSicWp0gwXKUIKdCkr7fVh6sKBix4YHZvUD0RLcZpToJvZ7Wa2z8wOmtl9Mzz+S2b2spnFzeyXs1+mSHaYGevb69h70R56QlMWpSjNGuhm5gceBu4ANgIfMrON5212DPgI8Gi2CxTJto3tdew9MUoyOfMlAMYn45qyKEVpLj30LcBB59xh51wMeAy4K3MD59wR59xOYPZfDxDx2Pq2WiKxBF2DkRkfj8QSGnKRojSXQF8OdGWsd6fb5s3MPm5m28xsW19f30JeQmTR1p85MDrzOPp4TD10KU55PSjqnHvEObfZObe5paUln28tcsa61lrMLjzTJTKpMXQpTnMJ9ONAZ8Z6R7pNpChVBv2saqpm74mZA31c0xalSM0l0F8C1prZKjMLAvcAW3NblkhurW+vZe8MUxedc5rlIkVr1kB3zsWBe4GngT3AN5xzu8zsQTO7E8DMrjOzbuADwBfNbFcuixZZrA1tdRw9HWE8fancaZPxJImkUw9ditKcuiHOuSeBJ89reyBj+SVSQzEiRWH6wOjeE6Ncu6LxTHtEl86VIqYzRaUsbWhPXdPl/HH06R67ZrlIMVKgS1la3lBJbSjA3vOmLg5PpK7CWF9Z4UVZIouiQJeylLoEQO0bpi4ORRToUrwU6FK2NqQvAeDc2UsATPfQG6qCXpUlsmAKdClb69vqGJuM0z04caZNQy5SzBToUrbWpw+MZg67DE3EAAW6FCcFupSt9W2pSwDszgj04Ykpgn4f4Qr905Dio2+tlK2qYIB1rbVsPzp4pm1kYor6qgr9/JwUJQW6lLVrVzTyyrEhEulrow9FpjTcIkVLgS5l7bqVSxibjLMvfV2XwUiMBgW6FCkFupS16dP+tx0dAODU6CRL60JeliSyYAp0KWsdjZW01oV46UhqHP3kcJTWurDHVYksjAJdypqZccOlzfzoQB+nxyYZjyVor1egS3FSoEvZe8fGVoYiUzy+LfVLi6ubazyuSGRhFOhS9m5a20Iw4OMvnt4HwLq2Wo8rElkYBbqUvepQgHdd2U7SwYqmKjoaK70uSWRBdBV/EeD+d22gJhTg7jct10lFUrQU6CJAc02IP737Cq/LEFkUDbmIiJQIBbqISIlQoIuIlAgFuohIiVCgi4iUCAW6iEiJUKCLiJQIBbqISIkw55w3b2zWBxxd4NObgf4slpMtqmt+VNf8FWptqmt+FlPXCudcy0wPeBboi2Fm25xzm72u43yqa35U1/wVam2qa35yVZeGXERESoQCXUSkRBRroD/idQEXoLrmR3XNX6HWprrmJyd1FeUYuoiIvFGx9tBFROQ8CnQRkRJR8IFuZh8ws11mljSzzec99kdmdtDM9pnZbRntt6fbDprZfXmo8XEz25G+HTGzHen2lWY2kfHY3+a6lvPq+hMzO57x/u/MeGzGfZenuj5nZnvNbKeZfcvMGtLtnu6vdA15/e5cpI5OM/uBme1Of/8/nW6/4Geax9qOmNlr6ffflm5bYmbPmNmB9H1jnmtal7FPdpjZiJl9xov9ZWZfMrNTZvZ6RtuM+8dS/jr9fdtpZpsW9ebOuYK+ARuAdcCzwOaM9o3Aq0AIWAUcAvzp2yFgNRBMb7Mxj/V+HnggvbwSeN3DffcnwB/M0D7jvstjXe8AAunlh4CHCmR/efrdOa+WdmBTerkW2J/+3Gb8TPNc2xGg+by2PwfuSy/fN/2Zevg5ngBWeLG/gF8CNmV+ly+0f4B3At8FDLge+Nli3rvge+jOuT3OuX0zPHQX8JhzbtI593PgILAlfTvonDvsnIsBj6W3zTlL/RjlB4Gv5+P9FuFC+y4vnHP/4ZyLp1dfADry9d6z8Oy7cz7nXK9z7uX08iiwB1juRS1zdBfwlfTyV4C7PazlFuCQc26hZ6IvinPuh8DAec0X2j93AV91KS8ADWbWvtD3LvhAv4jlQFfGene67ULt+XATcNI5dyCjbZWZvWJmz5nZTXmqI9O96T/lvpTxZ7CX++h8HyXVQ5nm5f4qpP1yhpmtBN4E/CzdNNNnmk8O+A8z225mH0+3tTrnetPLJ4BWD+qadg/ndqq83l9w4f2T1e9cQQS6mX3PzF6f4eZJ72gmc6zxQ5z7ReoFLnHOvQn4LPComdXlsa6/AdYA16Rr+Xw233sRdU1vcz8QB/4p3ZTz/VVszKwG+FfgM865ETz8TDPc6JzbBNwBfMLMfinzQZcaS/BkPrSZBYE7gX9ONxXC/jpHLvdPIBcvOl/Oubcv4GnHgc6M9Y50GxdpX7DZajSzAPA+4NqM50wCk+nl7WZ2CLgM2LbYeuZaV0Z9/xf4Tnr1YvsuL3WZ2UeAdwO3pL/gedlfs8j5fpkPM6sgFfzEtYoAAAGsSURBVOb/5Jz7JoBz7mTG45mfad44546n70+Z2bdIDVWdNLN251xvesjgVL7rSrsDeHl6PxXC/kq70P7J6neuIHroC7QVuMfMQma2ClgLvAi8BKw1s1Xp/63vSW+ba28H9jrnuqcbzKzFzPzp5dXpGg/noZbp988ci3svMH3U/UL7Ll913Q78V+BO51wko93T/YV33503SB+P+Xtgj3PuCxntF/pM81VXtZnVTi+TOsD9Oqn99OH0Zh8Gvp3PujKc81ey1/srw4X2z1bgN9KzXa4HhjOGZuYvn0d/F3jE+L2kxpUmgZPA0xmP3U9qVsI+4I6M9neSmhVwCLg/T3X+A/C757W9H9gF7ABeBt6T5333j8BrwM70F6d9tn2Xp7oOkho33JG+/W0h7C+vvjsXqONGUn+W78zYT++82Geap7pWk5r982r6s7o/3d4EfB84AHwPWOLBPqsGTgP1GW1531+k/kPpBabS2fWxC+0fUrNbHk5/314jYybfQm469V9EpEQU85CLiIhkUKCLiJQIBbqISIlQoIuIlAgFuohIiVCgi4iUCAW6iEiJ+P9VWlySueUWagAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bM_SXaaGzzU1"},"source":["### Exercises"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RIn58-Siz0hO"},"source":["**function exploration**\n","\n","Use your code for the 2-layer neural network to explore how the number of neruons in the hidden layer affects the function that the neural network implements. Vary the number of hidden neurons between say 1 and 20 and look at a few example networks for each. Plot some results for various values.\n","\n","How does the number of hidden units effect the function that your neural net represents? Explain why this is is this?\n","\n","What range of outputs seems to be acheivable? Why is this? What kind of supervised learning task would this network be suited for because of this?\n","\n","Type your answer in a text box below."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4yIy_a2I0tnS"},"source":["**deep neural nets**\n","\n","Our neural network currently works for only a single hidden layer. This is a rather shallow network, and the next step is to extend it to a deep network that can handle any number of layers.\n","\n","To handle an arbitrary number of layers, switch to taking a 3-dimensional array for the layer weights -- now the first index will specify which layer the remaing 2-dimension subarray is associated with (e.g. `W[2]` would be the 2-d array of weights for the connections from layer 2 to layer 3). Similary, add another dimension to the biases to specify the layer, making $b$ a 2-dimensional array.\n","\n","Here you will probably need to use a for loop to iterate through the layers. You can continue to use the sigmoid activation function for all layers."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0D8I7im509tV","colab":{}},"source":["def deep_nn(X, W, b):\n","  '''\n","  forward pass for a deep neural net with an arbitrary number of layers\n","\n","  parameters\n","  ----------\n","  X (2d array):     input vectors\n","  W (3d array):     weight matrices -- W[layer, input, neuron]\n","  b (2d array):     biase vectors -- b[layer, neuron]\n","  '''\n","  pass"],"execution_count":null,"outputs":[]}]}